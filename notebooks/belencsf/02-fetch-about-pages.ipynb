{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from IPython.core.display import display, HTML\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = \"./../../data/archived_snapshots_urls.csv\"\n",
    "recorded_urls = pd.read_csv(TARGET_PATH)\n",
    "recorded_urls = recorded_urls[recorded_urls.archived_snapshots != '{}'].reset_index(drop=True)\n",
    "recorded_urls['archived_snapshots'] = recorded_urls.archived_snapshots.apply(ast.literal_eval)\n",
    "archived_snapshots_expanded = pd.json_normalize(recorded_urls.archived_snapshots)\n",
    "recorded_urls = recorded_urls.drop(columns=[\"archived_snapshots\"]).join(archived_snapshots_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f014a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76256cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_urls['closest.url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ec805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://web.archive.org/web/20221217083518/https://mastodon.xyz/about\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"de-DE,de;q=0.9,en;q=0.8\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "}\n",
    "response = requests.get(url, headers=headers, timeout=30, allow_redirects=True)\n",
    "response.raise_for_status()\n",
    "\n",
    "open(\"page.html\", \"w\", encoding=\"utf-8\").write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64143d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2ea4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = response.text.lower()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "out = {}\n",
    "\n",
    "# \"About\" (best available in this HTML): meta description + title\n",
    "title = soup.title.get_text(strip=True) if soup.title else None\n",
    "meta_desc = soup.select_one('meta[name=\"description\"]')\n",
    "about = meta_desc[\"content\"].strip() if meta_desc and meta_desc.has_attr(\"content\") else None\n",
    "\n",
    "out[\"About\"] = {\n",
    "    \"title\": title,\n",
    "    \"description\": about,\n",
    "}\n",
    "\n",
    "# Helper: try to find a section by heading text (works only if content exists in HTML)\n",
    "def extract_section_by_heading(heading_regex):\n",
    "    heading = soup.find(\n",
    "        lambda t: t.name in [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"]\n",
    "        and t.get_text(\" \", strip=True)\n",
    "        and re.search(heading_regex, t.get_text(\" \", strip=True), re.I)\n",
    "    )\n",
    "    if not heading:\n",
    "        return None\n",
    "\n",
    "    # Collect text until the next heading\n",
    "    parts = []\n",
    "    for sib in heading.find_all_next():\n",
    "        if sib.name in [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"] and sib is not heading:\n",
    "            break\n",
    "        if sib.name in [\"p\",\"li\",\"div\",\"section\"]:\n",
    "            txt = sib.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                parts.append(txt)\n",
    "\n",
    "    return \"\\n\".join(parts).strip() or None\n",
    "\n",
    "# Try to extract (will return None for your HTML)\n",
    "out[\"Server Rules\"] = extract_section_by_heading(r\"(server\\s*rules|regeln|rules)\")\n",
    "out[\"Moderated Servers\"] = extract_section_by_heading(r\"(moderated\\s*servers|moderated|domain\\s*blocks|blocklist|server\\s*block)\")\n",
    "\n",
    "# Show whether the HTML even contains those phrases anywhere\n",
    "full_text = soup.get_text(\" \", strip=True).lower()\n",
    "out[\"_contains_keywords\"] = {\n",
    "    \"rules_word_present\": \"rules\" in full_text or \"regeln\" in full_text,\n",
    "    \"moderated_word_present\": \"moderated\" in full_text,\n",
    "    \"domain_blocks_word_present\": \"domain_blocks\" in full_text,\n",
    "}\n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
